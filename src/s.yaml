edition: 1.0.0
name: fc-llm
vars:
  region: '{{ region }}'
  LLM_MODEL: '{{ llmModel }}' # 基础模型及配置路径
  APP_NAME: '{{ appName }}'   # chatglm2-6b、app-chatglm2-6b-langchain、app-chatglm2-6b-api

  service:
    name: '{{ serviceName }}'
    description: '将开源模型部署到函数计算'
    {{ if llmModel !== 'chatglm2-6b-int4' ||  appName !== 'chatglm2-6b'  }} 
    nasConfig: auto
    logConfig: auto
    vpcConfig: auto
    {{/if}}
    internetAccess: true
services:
  {{ if llmModel !== 'chatglm2-6b-int4' ||  appName !== 'chatglm2-6b'  }} 
  fc-model-download:
    component: fc
    actions:
      pre-deploy: 
        - run: npm i 
          path: ./code/source-code/download-model2nas
    props:
      region: ${vars.region} # 关于变量的使用方法，可以参考：https://www.serverless-devs.com/serverless-devs/yaml#变量赋值
      service: ${vars.service}
      function:
        name: "model-download"
        description: 'download model to nas'
        codeUri: './code/source-code/download-model2nas'
        runtime: nodejs16
        timeout: 600
        memorySize: 3072
        cpu: 2.0
        diskSize: 512
        instanceConcurrency: 1
        handler: index.handler
        environmentVariables:
          modelPath: ${vars.LLM_MODEL}
          region: ${vars.region}
  {{/if}}
  

  chatglm6b-server:  #容器服务
    component: fc
    {{ if llmModel !== 'chatglm2-6b-int4' ||  appName !== 'chatglm2-6b'  }} 
    actions:
      post-deploy: 
        - component: fc nas upload -r ./code/{{appName}} /mnt/auto/llm/{{appName}}
          path: ./ 
        - component: fc nas upload -r ./code/${vars.llmModel} /mnt/auto/llm/${llmModel}  # chatglm2-6b-int4 | chatglm2-6b
        {{ if  llmModel == 'chatglm2-6b-int4' }} 
        - component: fc invoke --service-name ${fc-model-download.output.service.name}  # int4的模型由于较小，可以放在部署态完成
            --function-name model-download
        {{ /if }}
    {{/if}}
    props:
      region: ${vars.region}
      service: ${vars.service}
      function:
        handler: index.handler
        timeout: 600
        diskSize: 10240
        caPort: 7860
        instanceType: fc.gpu.tesla.1
        runtime: custom-container
        cpu: 8
        customContainerConfig:
          args: ''
          accelerationType: Default
          image: 'registry.${vars.region}.aliyuncs.com/serverlessdevshanxie/llm:v12'
          accelerationInfo:
            status: Preparing
          command: ''
          webServerMode: true
        instanceConcurrency: 100
        memorySize: 32768
        environmentVariables:
          LLM_MODEL: ${vars.LLM_MODEL}
          APP_NAME: ${vars.APP_NAME}
        gpuMemorySize: 16384
        name: chatglm
        asyncConfiguration: {}
      triggers:
        - name: defaultTrigger
          description: ''
          type: http
          qualifier: LATEST
          config:
            methods:
              - GET
              - POST
              - PUT
              - DELETE
            authType: anonymous
            disableURLInternet: false
      customDomains:
        - domainName: auto
          protocol: HTTP
          routeConfigs:
            - path: /*

  chatglm-portal:
    component: fc
    actions:
       pre-deploy:
        - run: npm i
          path: ./code/portal-code
    props:
      region: ${vars.region}
      service: ${vars.service}
      function:
        name: chatglm-portal
        description: Native recording handler
        timeout: 3000
        layers:
          - acs:fc:cn-hangzhou:official:layers/Nodejs18/versions/1
        instanceType: c1
        runtime: custom.debian10
        instanceConcurrency: 5
        memorySize: 3072
        cpu: 2.0
        diskSize: 512
        environmentVariables:
          NODE_PATH: /opt/nodejs/node_modules
          PATH: >-
            /opt/nodejs18/bin::/usr/local/bin/apache-maven/bin:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/ruby/bin:/opt/bin:/code:/code/bin
          chatServerUrl: '${chatglm6b-server.output.url.custom_domain[0].domain}'
          adminUrl: ${fc-nas-admin2.output.url.custom_domain[0].domain}
          LLM_MODEL: ${vars.LLM_MODEL}
          APP_NAME: ${vars.APP_NAME}
        codeUri: ./code/portal-code
        caPort: 
      triggers:
        - name: defaultTrigger
          type: http
          config:
            authType: anonymous
            methods:
              - GET
              - POST
              - PUT
              - DELETE
              - HEAD
              - OPTIONS
      customDomains:
        - domainName: auto
          protocol: HTTP
          routeConfigs:
            - path: /*
              serviceName: ${vars.service.name}
              functionName:  chatglm-portal
